---
layout: post
title:  "经典激活函数介绍"
data: 星期一, 30. 十二月 2019 12:04下午 
categories: 视觉
tags: 基础结构
---

# 计算机视觉经典结构–激活函数

## 梯度消失与梯度爆炸
这个网络上的解释已经很多了，简答来说就是反向传播经过激活函数的时候，激活函数的导数常常是0-1之间的一个值，那么经过的层数多了之后，那么远离输出靠近输入的层的导数就很小，就无法更新。这就是梯度消失。而梯度爆炸的原因则是因为求导的时候设置的初始权重太大，经过多层传递之后就爆炸超出了存储范围。

##一、 sigmoid

![](https://github.com/LLLibra/LLLibra.github.io/raw/master/_posts/imgs/20191230-121641.png)

![](https://github.com/LLLibra/LLLibra.github.io/raw/master/_posts/imgs/20191230-121700.png)

#### 导数

![](https://github.com/LLLibra/LLLibra.github.io/raw/master/_posts/imgs/20191230-132806.png)

![](https://github.com/LLLibra/LLLibra.github.io/raw/master/_posts/imgs/20191230-132810.png)

##二、 ReLU

![](https://github.com/LLLibra/LLLibra.github.io/raw/master/_posts/imgs/20191230-123514.png)

![](https://github.com/LLLibra/LLLibra.github.io/raw/master/_posts/imgs/20191230-123521.png)

#### 导数

![](https://github.com/LLLibra/LLLibra.github.io/raw/master/_posts/imgs/20191230-123629.png)

![](https://github.com/LLLibra/LLLibra.github.io/raw/master/_posts/imgs/20191230-123637.png)

#### 分析：
ReLU直接将网络中小于0的特征置为0,会引起网络的稀疏性，意味着一层的输出会有很多0（死亡ReLU）。这能提升时间和空间复杂度方面的效率——常数值（通常）所需空间更少，计算成本也更低。Yoshua Bengio 等人发现 ReLU 这种分量实际上能让神经网络表现更好，而且还有前面提到的时间和空间方面的效率。

#### 优点：
1. 相比于 sigmoid，由于稀疏性，时间和空间复杂度更低；不涉及成本更高的指数运算；
2. 能避免梯度消失问题。

#### 缺点：
1. 引入了死亡 ReLU 问题，即网络的大部分分量都永远不会更新。但这有时候也是一个优势；
2. ReLU 不能避免梯度爆炸问题。

##三、 Leaky ReLU

![](https://github.com/LLLibra/LLLibra.github.io/raw/master/_posts/imgs/20191230-123315.png)

* α常见的取值是在 0.1 到 0.3 之间

>
根据α取值不同还有两个变种，PReLU是将α设置为可以学习的超参数一般设为0.25，不同通道使用不同α，RReLU则是将α在训练的时候设置为一定范围内的随机值，在测试的时候会取固定值，有一定正则效果。

![](https://github.com/LLLibra/LLLibra.github.io/raw/master/_posts/imgs/20191230-123320.png)

#### 导数：
![](https://github.com/LLLibra/LLLibra.github.io/raw/master/_posts/imgs/20191230-125628.png)

![](https://github.com/LLLibra/LLLibra.github.io/raw/master/_posts/imgs/20191230-125632.png)

#### 优点：
1. 由于不包含指数运算，所以计算速度比 ELU 快。

#### 缺点：
1. 无法避免梯度爆炸问题；
2. 神经网络不学习 α 值，多了超参数。
3. 在微分时，两部分都是线性的；而 ELU 的一部分是线性的，一部分是非线性的。


##四、 ELU

![](https://github.com/LLLibra/LLLibra.github.io/raw/master/_posts/imgs/20191230-123212.png)

* α常见的取值是在 0.1 到 0.3 之间

![](https://github.com/LLLibra/LLLibra.github.io/raw/master/_posts/imgs/20191230-123218.png)

#### 导数：
![](https://github.com/LLLibra/LLLibra.github.io/raw/master/_posts/imgs/20191230-125020.png)

![](https://github.com/LLLibra/LLLibra.github.io/raw/master/_posts/imgs/20191230-125027.png)

#### 优点：
能得到负值输出，这能帮助网络向正确的方向推动权重和偏置变化(ReLU就不行)

#### 缺点：
1. 由于包含指数运算，所以计算时间更长；
2. 无法避免梯度爆炸问题；
3. 神经网络不学习 α 值，多了一个超参数。

##五、 SELU
扩展型指数线性单元激活函数比较新，介绍它的论文包含长达 90 页的附录（包括定理和证明等）。

![](https://github.com/LLLibra/LLLibra.github.io/raw/master/_posts/imgs/20191230-123334.png)

当实际应用这个激活函数时，必须使用 lecun_normal 进行权重初始化。而且如果你想使用 dropout，你也必须使用名为 Alpha Dropout 的特殊版本。

![](https://github.com/LLLibra/LLLibra.github.io/raw/master/_posts/imgs/20191230-130212.png)

可以看到，它们的小数点后还有很多位，这是为了绝对精度。而且它们是预先确定的，也就是说我们不必担心如何为这个激活函数选取合适的 α 值。

![](https://github.com/LLLibra/LLLibra.github.io/raw/master/_posts/imgs/20191230-123339.png)

#### 导数：
![](https://github.com/LLLibra/LLLibra.github.io/raw/master/_posts/imgs/20191230-130417.png)

![](https://github.com/LLLibra/LLLibra.github.io/raw/master/_posts/imgs/20191230-130427.png)

#### 优点：
1. 内部归一化的速度比外部归一化快，这意味着网络能更快收敛；
2. 不可能出现梯度消失或爆炸问题

#### 缺点：
这个激活函数相对较新——需要更多论文比较性地探索其在 CNN 和 RNN 等架构中应用。

##六、 GELU

![](https://github.com/LLLibra/LLLibra.github.io/raw/master/_posts/imgs/20191230-123430.png)

![](https://github.com/LLLibra/LLLibra.github.io/raw/master/_posts/imgs/20191230-123434.png)

#### 导数
![](https://github.com/LLLibra/LLLibra.github.io/raw/master/_posts/imgs/20191230-131406.png)

![](https://github.com/LLLibra/LLLibra.github.io/raw/master/_posts/imgs/20191230-131411.png)

#### 优点：

1. 似乎是 NLP 领域的当前最佳；尤其在 Transformer 模型中表现最好；
2. 能避免梯度消失问题。

尽管是 2016 年提出的，但在实际应用中还是一个相当新颖的激活函数。





