---
layout: post
title:  "xgboost"
data: 星期二, 10. 三月 2020 01:20下午 
categories: 机器学习
tags: 专题
---
* 该模块会针对机器学习中的某一块知识做专题整理，也许会有些不足或者错误的地方，未来可能会作修改。

#机器学习专题4----xgboost

* XGBoost（eXtreme Gradient Boosting）全名叫极端梯度提升

## 什么是xgboost
1.xgboost作为boosting家族的一员，XGBoost 所应用的算法就是 GBDT（gradient boosting decision tree）的改进，既可以用于分类也可以用于回归问题中。

2.xgboost也是基于CART树，也是以负梯度为学习策略。

** 由于xgboost和GBDT十分相像，只是在一些细节地方做了修改，完整讲一遍会和上一专题有很多重复，所以这里我们只记录做了哪些改进 **

## xgboost的改进

### 添加了正则项
![](https://github.com/LLLibra/LLLibra.github.io/raw/master/_posts/imgs/20200310-140755.png)

其中T为叶子数目，w为每个叶子的最终输出，做了一个L2正则。

所以xgboost的损失函数就变成了下式

![](https://github.com/LLLibra/LLLibra.github.io/raw/master/_posts/imgs/20200310-140938.png)

### xgboost使用了二阶导数
我们知道GBDT在负梯度上只使用了一阶导数，而xgboost使用了泰勒展开中的二阶导数来对我们的损失函数进行了改写。
> #### 原本的损失函数
>
![](https://github.com/LLLibra/LLLibra.github.io/raw/master/_posts/imgs/20200310-141244.png)
>#### 泰勒展开公式
>
![](https://github.com/LLLibra/LLLibra.github.io/raw/master/_posts/imgs/20200310-141345.png)
>#### 修改后的损失函数
>
![](https://github.com/LLLibra/LLLibra.github.io/raw/master/_posts/imgs/20200310-141515.png)
>
![](https://github.com/LLLibra/LLLibra.github.io/raw/master/_posts/imgs/20200310-142059.png)为L关于f的一阶导数，![](https://github.com/LLLibra/LLLibra.github.io/raw/master/_posts/imgs/20200310-142128.png)为L关于f的二阶导数。
>#### 对于如何在一棵树上求该损失函数也作出了改进
>
从遍历每个样本改为遍历所有叶子
>
![](https://github.com/LLLibra/LLLibra.github.io/raw/master/_posts/imgs/20200310-141907.png)
>
1.n为训练样本数，T为叶子数
>
2.由于![](https://github.com/LLLibra/LLLibra.github.io/raw/master/_posts/imgs/20200310-142007.png)是常数，所以直接去除
>
3.![](https://github.com/LLLibra/LLLibra.github.io/raw/master/_posts/imgs/20200310-142204.png)表示所有属于该叶子节点的训练样本。
>#### 再次化简
>
![](https://github.com/LLLibra/LLLibra.github.io/raw/master/_posts/imgs/20200310-142324.png)
>
![](https://github.com/LLLibra/LLLibra.github.io/raw/master/_posts/imgs/20200310-142529.png)
>####由于我们要找到最优的W 所以求导 再次化简
>
![](https://github.com/LLLibra/LLLibra.github.io/raw/master/_posts/imgs/20200310-142736.png)

### xgboost修改了构建树时的特征判断标准
原本我们知道CART回归树是采用最小二乘暴力法来选择特征和最优切分点的。而现在我们知道树的判断标准（损失函数），那么我们完全可以按照这个标准选择树的分割特征。

![](https://github.com/LLLibra/LLLibra.github.io/raw/master/_posts/imgs/20200310-143923.png)

我们仍旧需要暴力枚举所有的分割情况，但是在计算导数和G和H的时候可以优化一下，少计算一些。

### xgboost也引进了随机森林的列特征抽样（Column Subsampling），防止过拟合

我们会在同一层的结点分割前先随机选一部分特征，遍历的时候只用遍历这部分特征就行了，不需要便利全部特征，不仅能降低过拟合，还能减少计算，这也是XGBoost异于传统GBDT的一个特性。

### Shrinkage（一个超参数）
相当于学习速率（xgboost中的）。XGBoost在进行完一次迭代后，会将叶子节点的权重乘上该系数，主要是为了削弱每棵树的影响，让后面有更大的学习空间。实际应用中，一般把eta设置得小一点，然后迭代次数设置得大一点。（补充：传统GBDT的实现也有学习速率）

### 处理缺失值
XGBoost考虑了训练数据为稀疏值的情况，可以为缺失值或者指定的值指定分支的默认方向，这能大大提升算法的效率，论文中“枚举”指的不是枚举每个缺失样本在左边还是在右边，而是枚举缺失样本整体在左边，还是在右边两种情况。分裂点还是只评估特征不缺失的样本，paper提到50倍。即对于特征的值有缺失的样本，XGBoost可以自动学习出它的分裂方向。


### XGBoost工具支持并行
我们知道，决策树的学习最耗时的一个步骤就是对特征的值进行排序（因为要确定最佳分割点），xgboost在训练之前，预先对数据进行排序，然后保存block结构，后面的迭代中重复的使用这个结构，大大减小计算量。这个block结构也使得并行称为了可能，在进行节点的分裂时，需要计算每个特征的增益，最终选增益最大的那个特征去做分裂，那么各个特征的增益计算就可以开多线程进行。

### 可并行的近似直方图法
树节点在进行分裂时，我们需要计算每个特征的每个分割点对应的增益，即用贪心法枚举所有可能的分割点。当数据无法一次载入内存或者在分布式情况下，贪心算法效率就会变得很低，所以xgboost还提出了一种可并行的近似直方图算法，用于高效地生成候选的分割点。大致的思想是根据百分位法列举几个可能成为分割点的候选者，然后从候选者中根据上面求分割点的公式计算找出最佳的分割点。


### 传统GBDT以CART作为基分类器，xgboost还支持线性分类器

## xgboost的优点
#### 优点（相对于GBDT）
1.防止过拟合：XGBoost借鉴随机森林算法，支持列抽样，同时XGBoost在每一步中引入缩减因子，降低单颗树对结果的影响，让后续模型有更大的优化空间，进一步防止过拟合。

2.减少计算量：XGBoost借鉴随机森林算法，支持列抽样，同时在选择计算特征增益的时候可以并行。

3.XGBoost对缺失值不敏感，能自动学习其分裂方向

4.GBDT的基分类器只支持CART树，而XGBoost支持线性分类器

** 残留问题： **
1.xgboost如何使用其他的线性分类器

2.可并行的近似直方图法是个什么原理

3.为什么CART决策树确定最佳分割点要排序,是为了方便计算G和H吗

