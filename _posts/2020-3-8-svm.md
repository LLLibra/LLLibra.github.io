---
layout: post
title:  "svm"
data: 星期五, 06. 三月 2020 01:29下午 
categories: 机器学习
tags: 专题
---
* 该模块会针对机器学习中的某一块知识做专题整理，也许会有些不足或者错误的地方，未来可能会作修改。

#机器学习专题1----支持向量机SVM


## 什么是支持向量机
支持向量机（support vector machines, SVM）是一种二分类模型，它的基本模型是定义在特征空间上的间隔最大的线性分类器，间隔最大使它有别于感知机;SVM的的学习策略就是间隔最大化，可形式化为一个求解凸二次规划的问题，也等价于正则化的合页损失函数的最小化问题。SVM的的学习算法就是求解凸二次规划的最优化算法。

## SVM算法原理

![](https://github.com/LLLibra/LLLibra.github.io/raw/master/_posts/imgs/20200308-151312.png)

### 问题陈述

我们先看特征只有二维的，即上面一个点即为x,颜色则代表了它的y值。

** 前提条件：**

训练数据集是线性可分的。

** 给出训练数据集 **

![](https://github.com/LLLibra/LLLibra.github.io/raw/master/_posts/imgs/20200308-131340.png)

![](https://github.com/LLLibra/LLLibra.github.io/raw/master/_posts/imgs/20200308-131444.png)

* 注意x为向量而不是一个数。

** 给出超平面： **

三个平面中间的那个为

![](https://github.com/LLLibra/LLLibra.github.io/raw/master/_posts/imgs/20200308-131722.png)

![](https://github.com/LLLibra/LLLibra.github.io/raw/master/_posts/imgs/20200308-151535.png)

![](https://github.com/LLLibra/LLLibra.github.io/raw/master/_posts/imgs/20200308-151717.png)

则![](https://github.com/LLLibra/LLLibra.github.io/raw/master/_posts/imgs/20200308-151739.png)

** 优化目标 **

我们假设上面一个平面是 =1的超平面，红色代表y=1,下面的超平面是=-1的超平面，绿色代表y=-1,那么我们希望的就是在这两个超平面上的点到中间超平面的距离最小（实际上两个超平面到中间平面的距离应该是一样的），也就是下面的d是最小的。

![](https://github.com/LLLibra/LLLibra.github.io/raw/master/_posts/imgs/20200308-152701.png)



最大化![](https://github.com/LLLibra/LLLibra.github.io/raw/master/_posts/imgs/20200308-135924.png)和最小化 ![](https://github.com/LLLibra/LLLibra.github.io/raw/master/_posts/imgs/20200308-135935.png)是等价的，这样做的目的是为了方便后面的求导等求解过程。

** 所以最后我们的问题就变成了  **

![](https://github.com/LLLibra/LLLibra.github.io/raw/master/_posts/imgs/20200308-133616.png)

### 推导过程（拉格朗日）

>
首先了解一下拉格朗日函数 ，是寻找多元函数在一组约束（可以是等式约束也可以是不等式约束）下的极值的方法。

![](https://github.com/LLLibra/LLLibra.github.io/raw/master/_posts/imgs/20200308-133958.png)

#### 由此我们可以把上述问题的拉格朗日函数写出： 


![](https://github.com/LLLibra/LLLibra.github.io/raw/master/_posts/imgs/20200308-134506.png)

![](https://github.com/LLLibra/LLLibra.github.io/raw/master/_posts/imgs/20200308-162245.png)

弱对偶：原问题的解会大于等于对偶问题

强对偶：原问题的解会等于对偶问题，但要满足强对偶，可以看出SVM很容易满足强对偶条件

（KKT的第二个条件如果不满足那么原始问题就不等于我们的minMaxL的问题了）


** 接下去求 **

![](https://github.com/LLLibra/LLLibra.github.io/raw/master/_posts/imgs/20200308-162714.png)

### 推导过程（SMO）
暂留，日后更新

## 非线性的SVM


### 正常的数据可线性分割 但是有异常值

![](https://github.com/LLLibra/LLLibra.github.io/raw/master/_posts/imgs/20200308-184434.png)


** 什么是软间隔 **
>
![](https://github.com/LLLibra/LLLibra.github.io/raw/master/_posts/imgs/20200308-232203.png)
>
![](https://github.com/LLLibra/LLLibra.github.io/raw/master/_posts/imgs/20200308-232232.png)


采用软间隔的方法，加入一些松弛变量，同时给松弛变量加一些惩罚因子，限制松弛的力度

如果给定的** 惩罚项系数越小 **,表示在模型构建的时候,就
** 允许存在越多的分类错误的样本 **,也就表示此时模型的准确率会比较低;

如果 ** 惩罚项系数越大 **,表示在模型构建的时候,就
**越不允许存在分类错误的样本 **,也就表示此时模型的准确率会比较高。

![](https://github.com/LLLibra/LLLibra.github.io/raw/master/_posts/imgs/20200308-184852.png)

![](https://github.com/LLLibra/LLLibra.github.io/raw/master/_posts/imgs/20200308-191444.png)

参考网址https://blog.csdn.net/chaipp0607/article/details/75949812

这个结果和正常SVM推导结果很相似，只是多了一些约束

** 注意：**

1.并非所有的样本点都有一个松弛变量与其对应。实际上只有“离群点”才有，或者也可以这么看，所有没离群的点松弛变量都等于0。

2.松弛变量的值实际上标示出了对应的点到底离群有多远，值越大，点就越远。

3.惩罚因子C是一个超参数

### 正常的数据无法分割（核函数）
对于一些看起来就完全无法分割的数据我们可以采用将数据映射到高维再解决的方法，也就是通过使用核函数。

举一个简单的例子：

下图中4个点原本是线性不可分的，且都是二维，而现在给他们加上一个维度，第三维在图中有三种尝试，但是只有xy是有用的，并且加了第三维度之后我们就可以将这些数据分离。

![](https://github.com/LLLibra/LLLibra.github.io/raw/master/_posts/imgs/20200309-100436.png)

那么现在这两个特征向量从二维变成了三维，我们求这两个向量的内积变得更加复杂（注意我们上面拉格朗日推导的最后一部分是要求双方的内积的）。同时其实我们最终需要的也只是两个向量映射到高维之后的内积（因为我们已经求出结果），并不需要真正的知道他们是怎么映射的。

同时我们发现由于多出来的维度都是是前面两个维度组合而成的，也就是高维的内积不过是低维的内积变得更加复杂一些。

>
你们可能不明白我的意思，那么我举一个例子，比如我原来的向量是（x,y）,
现在我将它映射为（x,y,x^2,y^2,xy）的5维空间
>
![](https://github.com/LLLibra/LLLibra.github.io/raw/master/_posts/imgs/20200309-102259.png)
>
![](https://github.com/LLLibra/LLLibra.github.io/raw/master/_posts/imgs/20200309-102357.png) 就可以看作一种核函数数，采用核函数的计算了比先映射到高维在计算的计算量不是小了很多，而且这个映射的维度也不算高，若再高了计算量的差别会更大。

#### 所以我们可以给出核函数的定义了，同时原本的推导也发生了变化
![](https://github.com/LLLibra/LLLibra.github.io/raw/master/_posts/imgs/20200309-100956.png)
 
 原本的推导发生的一些改变，即将两个向量的内积改为了用核函数。
 
![](https://github.com/LLLibra/LLLibra.github.io/raw/master/_posts/imgs/20200309-101608.png)

#### 几种常见的核函数

>
![](https://github.com/LLLibra/LLLibra.github.io/raw/master/_posts/imgs/20200309-101720.png)

>
![](https://github.com/LLLibra/LLLibra.github.io/raw/master/_posts/imgs/20200309-101735.png)

>
![](https://github.com/LLLibra/LLLibra.github.io/raw/master/_posts/imgs/20200309-101814.png)


## SVM的回归问题SVR
参考链接：

https://www.hrwhisper.me/machine-learning-support-vector-machine-2-kernel-function-regression/

https://www.cnblogs.com/kexinxin/p/9858496.html

SVM 算法定义拟合的方式：在距离 Margin 的区域内，尽量多的包含样本点；

![](https://github.com/LLLibra/LLLibra.github.io/raw/master/_posts/imgs/20200308-232637.png)

![](https://github.com/LLLibra/LLLibra.github.io/raw/master/_posts/imgs/20200308-232655.png)

#### 所以

![](https://github.com/LLLibra/LLLibra.github.io/raw/master/_posts/imgs/20200308-232534.png)

#### 有max不是处处可微， 通过前面软间隔的启发，加入松弛因子 

![](https://github.com/LLLibra/LLLibra.github.io/raw/master/_posts/imgs/20200308-232801.png)

#### 仍旧有绝对值，则平面上下各引入一个松弛因子

![](https://github.com/LLLibra/LLLibra.github.io/raw/master/_posts/imgs/20200308-232926.png)



>
残留问题
>
问题一：为什么SVM会满足KKT 的L关于X的导数为0的条件
问题二：SVR中的![](https://github.com/LLLibra/LLLibra.github.io/raw/master/_posts/imgs/20200308-232952.png)是否为超参数






